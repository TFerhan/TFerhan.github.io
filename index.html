
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script type="text/javascript" src="js/hidebib.js"></script>

<head>
    <meta http-equiv="Content-Style-Type" content="text/css">

  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-weight: 300;
      font-size: 16px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .instructorphoto img {
      width: 200px;
      border-radius: 200px;
      margin-bottom: 10px;
    }

    .container {
    	width: 1024px;
    	min-height: 200px;
    	margin: 0 auto; /* top and bottom, right and left */
    	border: 1px hidden #000;
    	/* border: none; */
    	text-align: center;
    	padding: 1em 1em 1em 1em; /* top, right, bottom, left */
    	color: #4d4b59;
    	background: #FFFFFF;
    }

    .paper {
    	clear: both;
      /* width: 600px; */
    	margin: 0 auto; /* top and bottom, right and left */
    	border: 1px hidden #000;
    	/* border: none; */
    	text-align: left;
    	padding: 0.5em 1em 1em 0em; /* top, right, bottom, left */
    	color: #4d4b59;
    	background: #FFFFFF;
    }
    .news {
      margin: 0 auto; /* top and bottom, right and left */
      border: none;
      text-align: left;
      padding: -10em 1em 1em 1em; /* top, right, bottom, left */
    }

    div.paper pre {
      font-size: 0.8em;
    }

    li:not(:last-child) {
        margin-bottom: 8px;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/alexander_kirillov_small.jpg">
  <title>Alexander Kirillov</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Alexander Kirillov</name>
              </p>
              <p align="center">&nbsp;</p>
              <p align="center">I am working on multimodal AI models at <a href="https://thinkingmachines.ai/">Thinking Machines</a>. Previously, leading multimodal research group at <a href="https://openai.com/">OpenAI</a> working on Advanced Voice Mode and GPT-4o and befor that a research scientist at <a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research (FAIR)</a> working on Segment Anything (SAM).
              I received my PhD from <a href="https://www.uni-heidelberg.de/index_e.html">Heidelberg University</a>, Germany,
              under the supervision of <a href="https://hci.iwr.uni-heidelberg.de/vislearn/people/carsten-rother/">Carsten Rother</a>. I did my diploma at
              <a href="https://cs.msu.ru/en">Lomonosov Moscow State University</a>, Russia, where I worked with <a href="https://bayesgroup.ru/people/dmitry-vetrov/">Dmitry Vetrov</a>
              and <a href="http://www.mathnet.ru/php/person.phtml?&personid=30404&option_lang=eng">Alexander Dyakonov</a>.
              </p>
              <p align="center">
                <a href="mailto:alexander.n.kirillov@gmail.com" target="_blank"><img src="icons/mail.png" height="26"></a> &nbsp; &nbsp; &nbsp;
                <a href="https://scholar.google.com/citations?user=bHn29ScAAAAJ&hl" target="_blank"><img
                    src="icons/google_scholar.png" height="26"></a> &nbsp; &nbsp; &nbsp;
                <a href="https://x.com/_alex_kirillov_"><img src="icons/twitter.png" height="26"></a> &nbsp; &nbsp; &nbsp;
                <a href="https://www.linkedin.com/in/alexander-kirillov-08110749/" target="_blank"><img
                    src="icons/linkedin.png" height="26"></a>
              </p>
            </td>
            <td width="33%">
              <div class="instructorphoto"><img src="images/alexander_kirillov.jpg" id="alex"></div>
            </td>
          </tr>
        </table>
        <p align="center">&nbsp;</p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <p align="center">
            <name align="center">News</name>
          </p>
          <div class="news">
            <ul>
             <li><span> I have joined OpenAI to lead multimodal research group..
             <li><span> We released the <a href="https://segment-anything.com">segment anything</a> project. Check out our interactive <a href="https://segment-anything.com/demo">demo</a>, <a href="https://ai.facebook.com/datasets/segment-anything/">the new dataset</a> with 1.1B masks, and the <a href="https://github.com/facebookresearch/segment-anything">code</a>.
             <li><span> I will be serving as an area chair for CVPR 2023 and ICCV 2023
             <li><span> We received the PAMI Mark Everingham award for <a href="https://github.com/facebookresearch/detectron2">Detectron2</a>
             <li><span> We released the <a href="https://github.com/facebookresearch/detr">code</a> for DETR
            </ul>
          </div>
          <br>

          <p align="center">
            <name align="center">Publications</name>
          </p>

          <tr>
            <td valign="top" width="17%">
              <img src='images/segany.jpg' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/abs/2304.02643">
                <papertitle>Segment Anything</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Eric Mintun,
              Nikhila Ravi,
              Hanzi Mao,
              Chloe Rolland,
              Laura Gustafson,
              Tete Xiao,
              Spencer Whitehead,
              Alex C. Berg,
              Wan-Yen Lo,
              Piotr Doll&agrave;r,
              Ross Girshick
              <br>
              <em>ICCV</em>, 2023, <strong>Oral</strong>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2304.02643">paper</a> /
                <a href="https://segment-anything.com">project</a> /
                <a href="https://github.com/facebookresearch/segment-anything">code</a> /
                <a href="https://ai.facebook.com/datasets/segment-anything/">dataset</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/slip.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/2112.12750.pdf">
                <papertitle>SLIP: Self-supervision meets Language-Image Pre-training</papertitle>
              </a>
              <br>
              Norman Mu,
              <strong>Alexander Kirillov</strong>,
              David Wagner,
              Saining Xie
              <br>
              <em>ECCV</em>, 2022
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2112.12750">arxiv</a> /
                <a href="https://github.com/facebookresearch/SLIP">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/mask2former.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/2112.01527.pdf">
                <papertitle>Masked-attention Mask Transformer for Universal Image Segmentation</papertitle>
              </a>
              <br>
              Bowen Cheng,
              Ishan Misra,
              Alexander G. Schwing,
              <strong>Alexander Kirillov</strong>,
              Rohit Girdhar
              <br>
              <em>CVPR</em>, 2022
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2112.01527">arxiv</a> /
                <a href="https://github.com/facebookresearch/Mask2Former">code</a> /
                <a href="https://bowenc0221.github.io/mask2former">project</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/pointsup.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/2104.06404.pdf">
                <papertitle>Pointly-Supervised Instance Segmentation</papertitle>
              </a>
              <br>
              Bowen Cheng,
              Omkar Parkhi,
              <strong>Alexander Kirillov</strong>
              <br>
              <em>CVPR</em>, 2022, <strong>Oral</strong>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2104.06404">arxiv</a> /
                <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/PointSup">code</a> /
                <a href="https://bowenc0221.github.io/point-sup">project</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/trackformer.jpg' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/2101.02702.pdf">
                <papertitle>TrackFormer: Multi-Object Tracking with Transformers</papertitle>
              </a>
              <br>
              Tim Meinhardt,
              <strong>Alexander Kirillov</strong>,
              Laura Leal-Taix&eacute;,
              Christoph Feichtenhofer
              <br>
              <em>CVPR</em>, 2022
              <br>
              <div class="paper" id="trackformer">
                <a href="https://arxiv.org/abs/2101.02702">arxiv</a> /
                <a href="https://github.com/timmeinhardt/trackformer">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/pointcontrast.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/2202.04639.pdf">
                <papertitle>Point-Level Region Contrast for Object Detection Pre-Training</papertitle>
              </a>
              <br>
              Yutong Bai,
              Xinlei Chen,
              <strong>Alexander Kirillov</strong>, 
              Alan Yuille, 
              Alexander C Berg
              <br>
              <em>CVPR</em>, 2022, <strong>Oral</strong>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2202.04639">arxiv</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/maskformer.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/2107.06278.pdf">
                <papertitle>Per-Pixel Classification is Not All You Need for Semantic Segmentation</papertitle>
              </a>
              <br>
              Bowen Cheng,
              Alexander G. Schwing,
              <strong>Alexander Kirillov</strong>
              <br>
              <em>NeurIPS</em>, 2021, <strong>Spotlight</strong>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2107.06278">arxiv</a> /
                <a href="https://github.com/facebookresearch/MaskFormer">code</a> /
                <a href="https://bowenc0221.github.io/maskformer">project</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/incbar2.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/2102.11273.pdf">
                <papertitle>On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness</papertitle>
              </a>
              <br>
              Eric Mintun,
              <strong>Alexander Kirillov</strong>,
              Saining Xie
              <br>
              <em>NeurIPS</em>, 2021
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2102.11273">arxiv</a> /
                <a href="https://github.com/facebookresearch/augmentation-corruption">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/boundary_iou.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/2103.16562.pdf">
                <papertitle>Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</papertitle>
              </a>
              <br>
              Bowen Cheng,
              Ross Girshick,
              Piotr Dollar,
              Alexander C. Berg,
              <strong>Alexander Kirillov</strong>,
              <br>
              <em>CVPR</em>, 2021
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2103.16562">arxiv</a> /
                <a href="https://github.com/bowenc0221/boundary-iou-api">code</a> /
                <a href="https://bowenc0221.github.io/boundary-iou/">project</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/detr.jpg' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/2005.12872.pdf">
                <papertitle>DETR: End-to-End Object Detection with Transformers</papertitle>
              </a>
              <br>
              Nicolas Carion,
              Francisco Massa,
              Gabriel Synnaeve,
              Nicolas Usunier,
              <strong>Alexander Kirillov</strong>,
              Sergey Zagoruyko
              <br>
              <em>ECCV</em>, 2020, <strong>Oral</strong>
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/abs/2005.12872">arxiv</a> /
                <a href="https://github.com/facebookresearch/detr">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/kirillov2019pointrend.jpg' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1912.08193.pdf">
                <papertitle>PointRend: Image Segmentation as Rendering</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Yuxin Wu,
              Kaiming He,
              Ross Girshick
              <br>
              <em>CVPR</em>, 2020, <strong>Oral</strong>
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/abs/1912.08193">arxiv</a> / 
                <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/xie2019.png' alt="game" width="160" height="140" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1904.01569.pdf">
                <papertitle>Exploring Randomly Wired Neural Networks for Image Recognition</papertitle>
              </a>
              <br>
              Saining Xie,
              <strong>Alexander Kirillov</strong>,
              Ross Girshick,
              Kaiming He
              <br>
              <em>ICCV</em>, 2019, <strong>Oral</strong>
              <br>
              <div class="paper" id="randwire">
                <a href="https://arxiv.org/abs/1904.01569">arxiv</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/kirillov2019b.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1901.02446.pdf">
                <papertitle>Panoptic Feature Pyramid Networks</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Ross Girshick,
              Kaiming He,
              Piotr Doll&agrave;r
              <br>
              <em>CVPR</em>, 2019, <strong>Oral</strong>
              <br>
              <div class="paper" id="panopticfpn">
                <a href="https://arxiv.org/abs/1901.02446">arxiv</a> / 
                <a href="https://github.com/facebookresearch/detectron2">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/kirillov2019a.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1801.00868.pdf">
                <papertitle>Panoptic Segmentation</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Kaiming He,
              Ross Girshick,
              Piotr Doll&agrave;r
              <br>
              <em>CVPR</em>, 2019
              <br>
              <div class="paper" id="panoptic">
                <a href="https://arxiv.org/abs/1801.00868">arxiv</a> /
                <a href="https://github.com/cocodataset/panopticapi">evaluation code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/Arnab2018.png' alt="game" width="160" height="130" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="http://www.robots.ox.ac.uk/~tvg/publications/2017/CRFMeetCNN4SemanticSegmentation.pdf">
                <papertitle>Conditional random fields meet deep neural networks for semantic segmentation: Combining probabilistic graphical models with deep learning for structured prediction</papertitle>
              </a>
              <br>
              Anurag Arnab&#42;,
              Shuai Zheng&#42;,
              Sadeep Jayasumana,
              Bernardino Romera-Paredes,
              M&aring;ns Larsson,
              <strong>Alexander Kirillov</strong>,
              Bogdan Savchynskyy,
              Carsten Rother,
              Fredrik Kahl,
              Philip Torr
              <br>
              <em>IEEE Signal Processing Magazine</em>, 2018
              <br>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/kirillov2017.png' alt="game" width="160" height="100" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1611.08272.pdf">
                <papertitle>InstanceCut: from Edges to Instances with MultiCut</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Evgeny Levinkov,
              Bjoern Andres,
              Bogdan Savchynskyy,
              Carsten Rother
              <br>
              <em>CVPR</em>, 2017
              <br>
              <div class="paper" id="instancecut">
                <a href="https://arxiv.org/abs/1611.08272">arxiv</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/michel2017.png' alt="game" width="160" height="80" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1612.02287.pdf">
                <papertitle>Global hypothesis generation for 6D object pose estimation</papertitle>
              </a>
              <br>
              Frank Michel,
              <strong>Alexander Kirillov</strong>,
              Eric Brachmann,
              Alexander Krull,
              Stefan Gumhold,
              Bogdan Savchynskyy,
              Carsten Rother
              <br>
              <em>CVPR</em>, 2017
              <br>
              <div class="paper" id="pposeestimation">
                <a href="https://arxiv.org/abs/1612.02287">arxiv</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/levinkov2017a.png' alt="game" width="160" height="80" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1611.04399.pdf">
                <papertitle>Joint Graph Decomposition & Node Labeling: Problem, Algorithms, Applications</papertitle>
              </a>
              <br>
              Evgeny Levinkov,
              Jonas Uhrig,
              Siyu Tang,
              Mohamed Omran,
              Eldar Insafutdinov,
              <strong>Alexander Kirillov</strong>,
              Carsten Rother,
              Thomas Brox,
              Bernt Schiele,
              Bjoern Andres
              <br>
              <em>CVPR</em>, 2017
              <br>
              <div class="paper" id="multicut">
                <a href="https://arxiv.org/abs/1611.04399">arxiv</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/jafari2017.png' alt="game" width="160" height="80" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1702.08009.pdf">
                <papertitle>Analyzing Modular CNN Architectures for Joint Depth Prediction and Semantic Segmentation</papertitle>
              </a>
              <br>
              Omid Hosseini Jafari,
              Oliver Groth,
              <strong>Alexander Kirillov</strong>,
              Michael Ying Yang,
              Carsten Rother
              <br>
              <em>ICRA</em>, 2017
              <br>
              <div class="paper" id="modularcnn">
                <a href="https://arxiv.org/abs/1702.08009">arxiv</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/levinkov2017b.png' alt="game" width="160" height="80" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://www.dropbox.com/s/nzmeotf42vquwew/levinkov2017b.pdf?dl=0">
                <papertitle>A Comparative Study of Local Search Algorithms for Correlation Clustering</papertitle>
              </a>
              <br>
              Evgeny Levinkov,
              <strong>Alexander Kirillov</strong>,
              Bjoern Andres
              <br>
              <em>GCPR</em>, 2017
              <br>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/kirillov2015parametric.png' alt="game" width="160" height="120" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1606.07015.pdf">
                <papertitle>Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Alexander Shekhovtsov,
              Carsten Rother, Bogdan
              Savchynskyy
              <br>
              <em>NIPS</em>, 2016
              <br>
              <div class="paper" id="diversitysubmodular">
                <a href="https://arxiv.org/abs/1606.07015">arxiv</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/kirillov2016shape.png' alt="game" width="160" height="120" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="http://www.bmva.org/bmvc/2016/papers/paper088/paper088.pdf">
                <papertitle>Deep Part-Based Generative Shape Model with Latent Variables</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Mikhail Gavrikov,
              Ekaterina Lobacheva,
              Anton Osokin,
              Dmitry Vetrov
              <br>
              <em>BMVC</em>, 2016
              <br>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/kirillov2016cnncrf.png' alt="game" width="160" height="80" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://arxiv.org/pdf/1511.05067.pdf">
                <papertitle>Joint Training of Generic CNN-CRF Models with Stochastic Optimization</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Dmitrij Schlesinger,
              Shuai Zheng,
              Bogdan Savchynskyy,
              Philip H.S. Torr,
              Carsten Rother
              <br>
              <em>ACCV</em>, 2016
              <br>
              <div class="paper" id="cnncrf">
                <a href="https://arxiv.org/abs/1511.05067">arxiv</a>
              </div>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/kirillov2015submodular.png' alt="game" width="160" height="140" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="https://papers.nips.cc/paper/5977-m-best-diverse-labelings-for-submodular-energies-and-beyond.pdf">
                <papertitle>M-Best-Diverse Labelings for Submodular Energies and Beyond</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Dmitrij Schlesinger,
              Dmitry P Vetrov,
              Carsten Rother,
              Bogdan Savchynskyy
              <br>
              <em>NIPS</em>, 2015
              <br>
            </td>
          </tr>

          <tr>
            <td valign="top" width="17%">
              <img src='images/kirillov2015diversity.png' alt="game" width="160" height="100" style="border-style: none">
            </td>
            <td valign="top" width="83%">
              <a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Kirillov_Inferring_M-Best_Diverse_ICCV_2015_paper.pdf">
                <papertitle>Inferring M-Best Diverse Labelings in a Single One</papertitle>
              </a>
              <br>
              <strong>Alexander Kirillov</strong>,
              Bogdan Savchynskyy,
              Dmitrij Schlesinger,
              Dmitry P Vetrov,
              Carsten Rother,
              <br>
              <em>ICCV</em>, 2015
              <br>
            </td>
          </tr>
        </table>
        <p align="center">&nbsp;</p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <p align="center">
            <name align="center">Talks</name>
          </p>
          <tr>
            <td valign="middle" width="100%">
              <a href="https://youtu.be/7WcRfHXY_lw">
                <papertitle>Unification of Image-level Segmentation Tasks</papertitle>
              </a>
              <br>
              6th Workshop on Benchmarking Multi-Target Tracking at ICCV 2021
            </td>
          </tr>
          <tr>
            <td valign="middle" width="100%">
              <a href="https://youtu.be/QCtHGT68RIU">
                <papertitle>Pixel-level Recogition</papertitle>
              </a>
              <br>
              Tutorial on <em>Visual Recognition and Beyond</em> at CVPR 2020
            </td>
          </tr>
          <tr>
            <td valign="middle" width="100%">
              <a href="https://www.dropbox.com/s/t6tg87t78pdq6v3/cvpr19_tutorial_alexander_kirillov.pdf?dl=0">
                <papertitle>Panoptic Segmentation: Task and Approaches</papertitle>
              </a>
              <br>
              Tutorial on <em>Visual Recognition and Beyond</em> at CVPR 2019
            </td>
          </tr>
          <tr>
            <td valign="middle" width="100%">
              <a href="https://www.dropbox.com/s/cuglze46j4btnmd/panoptic_eccv18.pdf?dl=0">
                <papertitle>Panoptic Segmentation: Unifying Semantic and Instance Segmentations</papertitle>
              </a>
              <br>
              Tutorial on <em>Visual Recognition and Beyond</em> at ECCV 2018
            </td>
          </tr>
          <tr>
            <td valign="middle" width="100%">
              <a href="http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf">
                <papertitle>COCO-stuff Challenge Winner Talk</papertitle>
              </a>
              <br>
              Joint Workshop of the COCO and Places Challenges at ICCV 2017
            </td>
          </tr>
          <tr>
            <td valign="middle" width="100%">
              <a href="https://computing.ece.vt.edu/~cvpr16diversitytutorial/CVPR_2016_Diversity_files/CVPR16-tutorial-single-model.pdf">
                <papertitle>Generating Diverse Solutions from a Single Model</papertitle>
              </a>
              <br>
              Tutorial on Diversity meets Deep Networks - Inference, Ensemble Learning, and Applications at CVPR 2016
            </td>
          </tr>
        </table>
        <p align="center">&nbsp;</p>
        <p align="center">&nbsp;</p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <div style="clear:both;">
                <p align="right"><font size="2"><a href="http://jonbarron.info">Thanks for the template</a></font></p><br />
              </div>
            </td>
          </tr>
        </table>
        </td>
    </tr>
  </table>

  <script xml:space="preserve" language="JavaScript">
  hideallbibs();
  </script>

</body>

</html>
